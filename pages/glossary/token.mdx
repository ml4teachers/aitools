import MDXImage from '/components/mdximage';


# Token

Ein **Token** ist eine gemeinsame Sequenz von Zeichen, die in einem Text gefunden wird. Modelle wie die GPT-Familie von OpenAI verarbeiten Text auf der Basis dieser Tokens. Diese Modelle verstehen die statistischen Beziehungen zwischen diesen Tokens und sind besonders gut darin, das nächste Token in einer Sequenz von Tokens zu generieren.

Die Anzahl der Tokens in einem Text ist wichtig, da sie die Grundlage für die Berechnung der Ausführungszeit und der Kosten einer Anfrage bildet. Im Allgemeinen entspricht ein Token etwa 4 Zeichen eines allgemeinen englischen Textes. Dies entspricht in etwa ¾ eines Wortes. Deshalb entsprechen etwa 100 Tokens ungefähr 75 Wörtern.

## OpenAI Tokenizer Plattform

Auf der [OpenAI Tokenizer Plattform](https://platform.openai.com/tokenizer) steht ein Werkzeug zur Verfügung, um zu verstehen, wie ein Textstück von der API in Tokens zerlegt wird und wie viele Tokens in diesem Textstück insgesamt enthalten sind. Hier ein Beispiel:

<MDXImage src="/screenshots/tokenizer-screenshot.png" alt="Dieser Beispieltext besteht aus 42 Tokens." />

Die Sprachmodelle von OpenAI verarbeiten streng genomenn also keinen Text, sondern eine Sequenz von Zahlen. Diese Sequenz wäre für unser Beispiel:

```
[35, 444, 263, 8255, 41271, 328, 83, 257, 3046, 11, 288, 562, 705, 8206, ...]
```